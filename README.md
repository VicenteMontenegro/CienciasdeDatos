# Examen final Ciencia de datos para la economía 

Iniciamos cargando la base de datos nba_logreg2.csv con la función de la librería pandas`pd.read_csv`, luego con la función `head` vemos las primeras 5 filas de nuestro df. Luego realizamos un análisis estadístico con la función `describe`, la cual nos muestra la cantidad de datos, media, desviación estándar, valores mínimos y máximos, y los 3 cuartiles de cada columna. Lo primero que debemos hacer es estandarizar nuestras variables continuas. Generamos un nuevo df instanciando y aplicando el fit_transform de los datos con las funciones `StandardScaler` y `fit_transform`. Ya con los datos estandarizados, debemos ver si en nuestros datos existe alguna relación entre las variables. Para eso podemos usar la correlación de Pearson, ver la cantidad de observaciones o realizar un test estadístico, este último con una prueba de hipótesis, donde la hipótesis nula es que no hay evidencia de correlación en nuestras variables y la hipótesis alternativa es que sí hay correlación en nuestras variables. En este caso veremos la matriz de correlación entre las variables, donde podemos ver que existe algún tipo de relación entre las variables. Ya con esto, podemos empezar con nuestro PCA. Continuando con lo anterior, el índice de KMO también nos dice si nuestras variables sirven para el análisis de componentes principales a partir de los autovalores de nuestro df estandarizado. Con estos podemos determinar la cantidad de componentes según el índice KMO, el cual selecciona los componentes con autovalores mayores o iguales a 0,8. De esta forma obtenemos que 4 componentes son los óptimos para el PCA. Aplicamos la función `PCA` de sklearn y le indicamos la cantidad de componentes óptimos (4). Ahora, con este PCA con 4 componentes, entrenamos y transformamos nuestro df normalizado. De esta forma obtenemos un nuevo df reducido. Con la función PCA podemos obtener la varianza explicada de cada componente y con esta la varianza explicada acumulada. Es así como obtenemos que nuestros 4 componentes explican el 81,7% de la varianza original de los datos. Luego generamos un df con las componentes calculadas, para ver posteriormente con la función `.corr` y usamos el método de Pearson. De esta forma vemos que ya no hay problema de colinealidad. Tras esto, generamos un nuevo df con las covarianzas de cada columna/variable con cada componente. Generamos un nuevo df con el valor de las componentes principales para cada uno de los individuos de nuestro df original.

A continuación, usaremos el algoritmo Lazy Classifier, algoritmo que utiliza nuestros datos y les aplica distintos modelos automatizados de clasificación con tan solo segmentar los datos e instanciar el modelo. Luego, este algoritmo se encarga de entrenar los modelos y entregarnos los resultados de los distintos modelos con nuestros datos, ordenándolos desde el que tuvo mejor desempeño con nuestros datos hasta el modelo que tuvo peor desempeño. Para entrar en contexto, al seleccionar las columnas relevantes, en la variable y seleccionamos la columna 'TARGET_5Yrs', la cual tiene dos valores: 1 o 0. Si es 1, es un jugador que lleva más de 5 años jugando en la NBA, y si es 0, lleva menos de 5 años jugando en la NBA. Este contexto nos servirá para entender nuestras predicciones. El modelo que tuvo una mejor tasa de aciertos (accuracy) fue el modelo SGDClassifier. De los mejores modelos que vimos en clases y se encuentran dentro de los con mejor accuracy con nuestros datos son: 1. Derivados del modelo de Support Vectors Machine, modelo que busca maximizar el margen, es decir, maximizar la distancia entre los vectores de soporte. 2. Logistic Regression: Predecir la probabilidad de que una muestra pertenezca a una clase específica. 3. Random Forest Classifier: modelo con muy buen rendimiento para la clasificación y escalabilidad. La idea de este modelo es un bosque de árboles de decisión múltiples que se promedian para crear un modelo más robusto, obteniendo un mejor rendimiento de generalización y siendo menos probable el sobreajuste. 4. AdaBoost: Entrena secuencialmente clasificadores débiles, enfocándose en las instancias clasificadas incorrectamente en cada iteración para mejorar el rendimiento del modelo. 5. KNN (K-Nearest Neighbors): modelo muy sencillo, predice los datos a partir de los valores más cercanos en una cierta distancia a través de una votación por mayoría. 6. Decision Tree: busca separar los datos en dos clases basado en sus características. 7. XGBoost: su objetivo es mejorar la precisión predictiva mediante árboles de decisión secuenciales, corrigiendo errores iterativamente y optimizando la velocidad de entrenamiento con paralelización y técnicas avanzadas.

Para la última parte del examen, debemos utilizar uno de los modelos de clasificación vistos en clases. En este caso, usaré el modelo Random Forest Classifier. Instanciamos el algoritmo de RFC con la función `RandomForestClassifier`, para luego aplicar el modelo en nuestros datos. Luego de esto, generamos las predicciones de nuestros datos, para ver la matriz de confusión a partir de nuestras predicciones. Después de esto, generamos dos funciones que nos facilitarán ver las métricas del modelo con la predicción de nuestros datos. Obtenemos el reporte de la predicción de nuestro modelo, para buscar los mejores hiperparámetros para nuestro modelo. Tras obtener estos nuevos parámetros, entrenamos nuevamente el modelo con ellos. Obtenemos los reportes de la predicción:

-Accuracy: 67% de acierto en las predicciones correctas.
-Precisión: El 55% de los datos positivos y el 72% de los datos negativos fueron clasificados correctamente. Es decir, el modelo predice mejor para los jugadores que han jugado más de 5 años en la NBA (72%) que para los que han jugado menos de 5 años (55%).
-Recall: El modelo identifica correctamente en un 45% a los jugadores que han jugado menos de 5 años y en un 79% a los jugadores que han jugado más de 5 años.
-F1 Score: Mide el equilibrio entre la precisión y el recall. En este caso, existe un buen equilibrio para los jugadores con más de 5 años (75%) y un moderado equilibrio para los que llevan menos de 5 años (50%), es decir, el modelo clasifica mejor a los jugadores que han jugado más de 5 años.
-Cohen Kappa: La puntuación de 0.2499 implica que aunque el modelo no tenga una concordancia perfecta entre la clasificación del modelo y la clasificación real, sí está superando la coincidencia aleatoria, proporcionando un nivel moderado de fiabilidad, es decir, existe una clasificación al azar.
Podemos concluir que el modelo predice moderadamente bien nuestros datos, pero no perfectamente. Como vimos al aplicar el Lazy Classifier, existen muchos otros modelos que estaban por sobre el Random Forest Classifier a la hora de predecir nuestros datos, pero no hace una mala predicción.
