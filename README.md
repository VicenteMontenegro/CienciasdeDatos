# Examen final Ciencia de datos para la economía 

Iniciamos cargando la base de datos nba_logreg2.csv con la funcion de la libreria pandas 'pd.read_csv', luego con la funcion 'head' vemos las primeras 5 filas de nuestro df.
Luego realizamos un analisis estadistico con la funcion 'describe' la cual nos muestra la cantidad de datos, media, desviación estándar, valores minimos y maximos, y los 3 cuartiles de cada columna.Lo primero que debemos hacer es estandarizar nuestras varibales continuas, generamos un nuevo df instanciando y aplicando el fit_transfer de los de los datos con las funciones 'StandardScaler' y 'fit_transfer'. Ya con los datos estandarizados debemos ver si en nuestros datos existe alguna relacion entre las variables, para eso podemos usar la correalcion de pearson, ver la cantidad de observaciones o realizar un test estadistico,este ultimo con una pruebva de hipotesis, donde la hipotesis nula es que no hya evidencia de correalcion en nuestras variables y la hipotesis alternativa que si hay correlacion en nuestras variables. En este caso veremos la matriz de correalcion entre las variables, donde podemos ver que existe algun tipo de realcion entre las variables, ya con esto podemos empezar con nuestro PCA. Continuando con lo anterior el indice de KMO tambien nos dice si nuestras varibales sirven para el analisis de componentens principales a partir de los autovalores de nuestro df estandarizado, con estos podemos determinar la cantidad de componentes segun el indice KMO, el cual selecciona los componentes con autovalores mayores o iguales a 0,8. De esta forma obtenemos que 4 componentes son los optimos para el PCA. Aplicamos la funcion 'PCA' de sklearn y le indicamos la cantidad de componentes optimos(4). Ahora con este PCA con 4 componentes, entrenamos y transformamos nuestro df normalizado, de estaf orma obtenemos un nuevo df reducido. Con la funcion PCA podemos obtener la varianza explicada de cada componente y con esta la varianza explicada acumulada, es asi como obtenemos que nuestros 4 componentes explican el 81,7% de la varianza original de los datos originales. Luego generamos un df con las componentes calculadas, para ver posteriormente con la funcion '.cor' y usamos el metodo de pearson, de esta forma vemos que ya no hay problema de collinealidad. Tras esto, generamos un nuevo df con las covarianzas de cada columna/variable con cada componente. Generamos un nuevo df con el valor de las componentes principales para cada uno de los individuos de nuestro df original.
A continuacion usaremos el algoritmo Lazy Clasiffier, algoritmo que utiliza nuestros datos y le aplica distintos modelos automatizados de clasificacion con tan solo que segmentemos los datos e instanciemos el modelo, luego este algoritmo se encarga entrenar los modelos y entregarnos los resultados de los distintos modelos con nuestros datos, ordenandolos desde el que tuvo mejor desempeño con nuestros datos hasta el modelo que tuvo peor desempeño. Para entrar en conexto, al seleccionar las columnas relevantes, en la variable y, seleccionamos la columna 'TARGET_5Yrs', la cual tiene tiene dos valores, 1 o 0, si es 1 es un jugador que lleva mas de 5 años jugando en la NBA y si es 0 lleva menos de 5 años jugando en la NBA. Este contexto nos servira para entender nuestras predicciones.El modelo que tuvo una mejor tasa de aciertos(acurracy) fue el modelo SGDClassifier, de los mejores modelos que vimos en clases y se encuentran dentros de los con mejor acurracy con nuestros datos son: 1-Derivados del modelo de SupportVectorsMachine, modelo que busca busca maximizar el margen, esto quiere decir maximizar la distancia entre los vectores de soporte. 2-Logistic Regression: Predecir la probabilidad de que una muestra pertenezca a una clase específica. .3- Random Forest Classifier: modelo con muy buen rendimiento para la clasificacion y escalabilidad. La idea de este modelo es un bosque de arboles de decision multiples que se promedian para crear un modelo mas robusto, obteniendo un mehor rendimineto de generalizacion y sea menos probable el sobreajuste. 4- AdaBoost: Entrena secuencialmente clasificadores débiles, enfocándose en las instancias clasificadas incorrectamente en cada iteración para mejorar el rendimiento del modelo. 5- KNN (K-Neighbors Nearest): modelo muy sencillo, predice los datos a partir de los valores más cercanos en una cierta distancia a traves de una votacion por mayoria.  6- Decision Tree: busca separar los datos en dos clases basado en sus caracteristicas. 7- XGBoost: su objetivo es mejorar la precisión predictiva mediante árboles de decisión secuenciales, corrigiendo errores iterativamente y optimizando la velocidad de entrenamiento con paralelización y técnicas avanzadas.
Para la ultima parte del examen debemos utilizar uno de los modelos de clasificacion vistos en clases, en este caso usare el modelo Random Forest Classifier. Instanciamos el algoritmo de RFC con la funcion ´RandomForestClassifier´, para luego aplicar el modelo en nuestros datos. Luego de esto generamos las prediciones de nuestros datos, para ver la matriz de confusion a partir de nuestras predicciones. Despues de esto generamos dos funciones que nos facilitara ver las metricas del modelo con la prediccion de nuestros datos. Obtenemos el reporte de la prediccion de nuestro modelo, para buscar los mejores hiperparametros para nuestro modelo. Tras obtener estos nuevos parametros entrenamos nuevamente el modelo con ellos. Obetenemos los reportes de la prediccion: -Accuracy: 67% de acierto en las predicciones correctas. -Precisión: El 55% de los datos positivos y el 72% de los datos negativos fueron calsificados correctamente. Es decir, el modelo predice mejor para los jugadores que han jugado mas de 5 años en la NBA(72%) que para los que han jugado menos de 5 años(55%) -Recall: El modelo identifica correctamente en un 45% a los jugadores que han jugado menos de 5 años y en un 79% a los jugadores que han jugado mas de 5 años. -F1 Score: Mide el equilibrio entre la precision y el recall, en este caso existe un buen equilibrio para los jugadores con mas de 5 años(75%) y un moderado equilibrio para los que llevan menos de 5 años(50%), es decir el modelos clasifica mejor a los jugadores que han jugado mas de 5 años. -Cohen Kappa: la puntuacion de 0.2499 implica que aunque el modelo no tenga una concordacia perfecta entre la clasificacion del modelo y la clasificacion real, sí está superando la coincidencia aleatoria, proporcionando un nivel moderado de fiabilidad,es decir, existe una clasificacion al azar. Podemos concluir que el modelo predice moderadamente bien nuestros datos, pero no perfectamente. Como vimos al aplicar el Lazzy Clasiffier existen muhcos otros modelos que estaban por sobre el Random Forest Clasiffier a la hora de predecir nuestros datos, pero no hace una mala prediccion.